"""
Test Script: Comprehensive Metric Normalization System Test
File: test_metric_normalization_system.py
Description: 
  2-1-3ã€ŒæŒ‡æ¨™ã®æ­£è¦åŒ–æ‰‹æ³•ã®è¨­è¨ˆã€ã‚·ã‚¹ãƒ†ãƒ ã®åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ
  è¨­å®šã€ã‚¨ãƒ³ã‚¸ãƒ³ã€ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®çµ±åˆå‹•ä½œã‚’æ¤œè¨¼

Author: imega
Created: 2025-07-10
Modified: 2025-07-10
"""

import logging
import numpy as np
import pandas as pd
from pathlib import Path
import json
from typing import Dict, Any

# å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from config.metric_normalization_config import MetricNormalizationConfig, NormalizationParameters
from config.metric_normalization_engine import MetricNormalizationEngine
from config.metric_normalization_manager import MetricNormalizationManager

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

def create_test_data() -> Dict[str, Dict[str, np.ndarray]]:
    """ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
    np.random.seed(42)
    
    # æˆ¦ç•¥ãƒ‡ãƒ¼ã‚¿
    strategies_data = {
        "trend_following": {
            "sharpe_ratio": np.random.normal(1.2, 0.6, 100),
            "sortino_ratio": np.random.normal(1.5, 0.8, 100),
            "profit_factor": np.random.exponential(2.0, 100),
            "win_rate": np.random.beta(2, 2, 100),
            "max_drawdown": np.random.exponential(0.15, 100) * -1,  # è² ã®å€¤
            "total_return": np.random.normal(0.15, 0.25, 100)
        },
        "mean_reversion": {
            "sharpe_ratio": np.random.normal(0.8, 0.4, 100),
            "sortino_ratio": np.random.normal(1.0, 0.5, 100),
            "profit_factor": np.random.exponential(1.5, 100),
            "win_rate": np.random.beta(1.5, 2, 100),
            "max_drawdown": np.random.exponential(0.12, 100) * -1,
            "total_return": np.random.normal(0.10, 0.20, 100)
        },
        "arbitrage": {
            "sharpe_ratio": np.random.normal(2.0, 0.3, 100),
            "sortino_ratio": np.random.normal(2.5, 0.4, 100),
            "profit_factor": np.random.exponential(3.0, 100),
            "win_rate": np.random.beta(3, 1, 100),
            "max_drawdown": np.random.exponential(0.05, 100) * -1,
            "total_return": np.random.normal(0.08, 0.10, 100)
        }
    }
    
    return strategies_data

def test_config_system():
    """è¨­å®šã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== Testing Configuration System ===")
    
    try:
        # è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆ
        config = MetricNormalizationConfig()
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šã®ç¢ºèª
        sharpe_params = config.get_normalization_parameters("sharpe_ratio")
        logger.info(f"âœ“ Sharpe ratio config: {sharpe_params.method}")
        
        # æˆ¦ç•¥åˆ¥ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã®è¿½åŠ 
        success = config.add_strategy_override(
            "test_strategy",
            {
                "sharpe_ratio": {
                    "method": "robust",
                    "target_range": (0.0, 1.0),
                    "outlier_handling": "transform"
                }
            },
            notes="Test strategy override"
        )
        logger.info(f"âœ“ Strategy override added: {success}")
        
        # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰è¨­å®šã®ç¢ºèª
        override_params = config.get_normalization_parameters("sharpe_ratio", "test_strategy")
        logger.info(f"âœ“ Override config: {override_params.method}")
        
        # è¨­å®šæ¤œè¨¼
        validation = config.validate_config()
        logger.info(f"âœ“ Config validation: {validation['valid']}")
        
        # è¨­å®šè¦ç´„
        summary = config.get_config_summary()
        logger.info(f"âœ“ Config summary: {len(summary['global_metrics'])} metrics, {len(summary['strategy_overrides'])} overrides")
        
        return True
        
    except Exception as e:
        logger.error(f"âœ— Config system test failed: {e}")
        return False

def test_engine_system():
    """ã‚¨ãƒ³ã‚¸ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== Testing Engine System ===")
    
    try:
        # ã‚¨ãƒ³ã‚¸ãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆ
        config = MetricNormalizationConfig()
        engine = MetricNormalizationEngine(config)
        
        # å˜ä¸€æŒ‡æ¨™æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ
        test_data = np.random.normal(1.0, 0.5, 50)
        result = engine.normalize_metric(test_data, "sharpe_ratio")
        logger.info(f"âœ“ Single metric normalization: {result.success}, confidence: {result.confidence_score:.3f}")
        
        # ä¸€æ‹¬æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ
        batch_data = {
            "sharpe_ratio": np.random.normal(1.0, 0.5, 50),
            "profit_factor": np.random.exponential(1.5, 50),
            "win_rate": np.random.beta(2, 2, 50)
        }
        batch_results = engine.batch_normalize(batch_data)
        success_count = sum(1 for r in batch_results.values() if r.success)
        logger.info(f"âœ“ Batch normalization: {success_count}/{len(batch_data)} successful")
        
        # ã‚«ã‚¹ã‚¿ãƒ æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ
        custom_result = engine.normalize_metric(
            np.random.exponential(1.5, 50), 
            "profit_factor"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã‚«ã‚¹ã‚¿ãƒ æ‰‹æ³•ã‚’ä½¿ç”¨
        )
        logger.info(f"âœ“ Custom normalization: {custom_result.success}, method: {custom_result.method_used}")
        
        # åˆ©ç”¨å¯èƒ½æ‰‹æ³•ã®ç¢ºèª
        methods = engine.get_available_methods()
        custom_funcs = engine.get_custom_functions()
        logger.info(f"âœ“ Available methods: {len(methods)}, custom functions: {len(custom_funcs)}")
        
        return True
        
    except Exception as e:
        logger.error(f"âœ— Engine system test failed: {e}")
        return False

def test_manager_system():
    """ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== Testing Manager System ===")
    
    try:
        # ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆ
        manager = MetricNormalizationManager(integration_mode="metric_selection")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        test_strategies = create_test_data()
        
        # å˜ä¸€æˆ¦ç•¥æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ
        single_summary = manager.normalize_strategy_metrics(
            "trend_following",
            test_strategies["trend_following"],
            save_session=True
        )
        logger.info(f"âœ“ Single strategy normalization: {single_summary.success}")
        logger.info(f"  - Metrics processed: {len(single_summary.session_info.metrics_processed)}")
        logger.info(f"  - Success rate: {single_summary.session_info.success_rate:.3f}")
        logger.info(f"  - Processing time: {single_summary.session_info.total_processing_time:.3f}s")
        
        # ä¸€æ‹¬æˆ¦ç•¥æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ
        batch_summaries = manager.batch_normalize_strategies(test_strategies)
        successful_strategies = sum(1 for s in batch_summaries.values() if s.success)
        logger.info(f"âœ“ Batch strategies normalization: {successful_strategies}/{len(test_strategies)} successful")
        
        # æŒ‡æ¨™é¸å®šã‚·ã‚¹ãƒ†ãƒ ç”¨æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ
        selection_data = manager.normalize_for_metric_selection(
            test_strategies["arbitrage"]
        )
        logger.info(f"âœ“ Metric selection normalization: {len(selection_data)} metrics processed")
        
        # åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
        comprehensive_report = manager.generate_comprehensive_report(batch_summaries)
        logger.info(f"âœ“ Comprehensive report generated")
        logger.info(f"  - Overall success rate: {comprehensive_report['overall_performance']['average_success_rate']:.3f}")
        logger.info(f"  - Total metrics processed: {comprehensive_report['overall_performance']['total_metrics_processed']}")
        
        # å±¥æ­´å–å¾—ãƒ†ã‚¹ãƒˆ
        history = manager.get_normalization_history(days_back=1)
        logger.info(f"âœ“ History retrieval: {len(history)} sessions found")
        
        return True
        
    except Exception as e:
        logger.error(f"âœ— Manager system test failed: {e}")
        return False

def test_integration_features():
    """çµ±åˆæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== Testing Integration Features ===")
    
    try:
        # ç•°ãªã‚‹çµ±åˆãƒ¢ãƒ¼ãƒ‰ã§ã®ãƒ†ã‚¹ãƒˆ
        modes = ["scoring", "metric_selection", "standalone"]
        
        for mode in modes:
            try:
                manager = MetricNormalizationManager(integration_mode=mode)
                test_data = create_test_data()["trend_following"]
                
                summary = manager.normalize_strategy_metrics(
                    "integration_test",
                    test_data,
                    save_session=False
                )
                
                logger.info(f"âœ“ Integration mode '{mode}': {summary.success}")
                
                # çµ±åˆæƒ…å ±ã®ç¢ºèª
                if summary.scoring_integration:
                    logger.info(f"  - Scoring integration: {'success' if 'error' not in summary.scoring_integration else 'failed'}")
                if summary.selection_integration:
                    logger.info(f"  - Selection integration: {'success' if 'error' not in summary.selection_integration else 'failed'}")
                    
            except Exception as e:
                logger.warning(f"Integration mode '{mode}' test failed: {e}")
        
        return True
        
    except Exception as e:
        logger.error(f"âœ— Integration features test failed: {e}")
        return False

def test_performance_evaluation():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== Testing Performance Evaluation ===")
    
    try:
        import time
        
        # å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        large_data = {}
        for i in range(10):  # 10æˆ¦ç•¥
            strategy_name = f"strategy_{i}"
            large_data[strategy_name] = {}
            for metric in ["sharpe_ratio", "profit_factor", "win_rate", "max_drawdown", "total_return"]:
                large_data[strategy_name][metric] = np.random.normal(0, 1, 1000)  # å„æŒ‡æ¨™1000ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ
        
        manager = MetricNormalizationManager()
        
        start_time = time.time()
        batch_summaries = manager.batch_normalize_strategies(large_data, save_sessions=False)
        processing_time = time.time() - start_time
        
        successful_strategies = sum(1 for s in batch_summaries.values() if s.success)
        total_metrics = sum(len(data) for data in large_data.values())
        
        logger.info(f"âœ“ Performance test completed:")
        logger.info(f"  - Strategies: {len(large_data)}")
        logger.info(f"  - Total metrics: {total_metrics}")
        logger.info(f"  - Successful strategies: {successful_strategies}")
        logger.info(f"  - Processing time: {processing_time:.3f}s")
        logger.info(f"  - Throughput: {total_metrics/processing_time:.1f} metrics/second")
        
        return True
        
    except Exception as e:
        logger.error(f"âœ— Performance evaluation test failed: {e}")
        return False

def generate_test_report(test_results: Dict[str, bool]):
    """ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
    logger.info("=== Test Results Summary ===")
    
    total_tests = len(test_results)
    passed_tests = sum(test_results.values())
    
    logger.info(f"Total Tests: {total_tests}")
    logger.info(f"Passed: {passed_tests}")
    logger.info(f"Failed: {total_tests - passed_tests}")
    logger.info(f"Success Rate: {passed_tests/total_tests*100:.1f}%")
    
    logger.info("\nDetailed Results:")
    for test_name, result in test_results.items():
        status = "âœ“ PASS" if result else "âœ— FAIL"
        logger.info(f"  {test_name}: {status}")
    
    # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    try:
        report_dir = Path("logs/metric_normalization/test_reports")
        report_dir.mkdir(parents=True, exist_ok=True)
        
        report_data = {
            "test_timestamp": pd.Timestamp.now().isoformat(),
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": total_tests - passed_tests,
            "success_rate": passed_tests/total_tests,
            "detailed_results": test_results
        }
        
        report_file = report_dir / f"test_report_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nTest report saved: {report_file}")
        
    except Exception as e:
        logger.warning(f"Failed to save test report: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    logger.info("Starting Comprehensive Metric Normalization System Test")
    logger.info("="*60)
    
    # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®å®Ÿè¡Œ
    test_results = {
        "Config System": test_config_system(),
        "Engine System": test_engine_system(),
        "Manager System": test_manager_system(),
        "Integration Features": test_integration_features(),
        "Performance Evaluation": test_performance_evaluation()
    }
    
    # çµæœãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
    generate_test_report(test_results)
    
    # çµ‚äº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    if all(test_results.values()):
        logger.info("\nğŸ‰ All tests passed! Metric Normalization System is working correctly.")
    else:
        logger.warning("\nâš ï¸ Some tests failed. Please review the detailed results above.")

if __name__ == "__main__":
    main()
