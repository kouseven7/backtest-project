"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¤œè¨¼
5-3-3 æˆ¦ç•¥é–“ç›¸é–¢ã‚’è€ƒæ…®ã—ãŸé…åˆ†æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ  è² è·ãƒ†ã‚¹ãƒˆç”¨

Author: imega
Created: 2025-07-24
Task: Load Testing for 5-3-3
"""

from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import json
import logging
import os
from datetime import datetime

# æ—¢å­˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from performance_monitor import PerformanceMetrics
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    @dataclass
    class PerformanceMetrics:
        execution_time: float
        peak_memory_mb: float
        avg_cpu_percent: float
        memory_growth_mb: float
        gc_collections: int
        start_memory_mb: float
        end_memory_mb: float
        thread_count: int
        process_count: int

@dataclass 
class BenchmarkThresholds:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–¾å€¤è¨­å®š"""
    max_time: float
    max_memory: float
    max_cpu_percent: float = 80.0
    max_memory_growth: float = 100.0  # MB
    max_gc_collections: int = 10

@dataclass
class ValidationResult:
    """æ¤œè¨¼çµæœ"""
    test_name: str
    passed: bool
    metrics: PerformanceMetrics
    thresholds: BenchmarkThresholds
    violations: List[str]
    score: float  # 0-100ã®ç·åˆã‚¹ã‚³ã‚¢

class BenchmarkValidator:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¤œè¨¼å™¨"""
    
    def __init__(self, config_file: Optional[str] = None):
        self.logger = logging.getLogger(__name__)
        self.benchmarks = self._load_benchmarks(config_file)
        self.validation_history: List[ValidationResult] = []
        
    def _load_benchmarks(self, config_file: Optional[str] = None) -> Dict[str, BenchmarkThresholds]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åŸºæº–å€¤èª­ã¿è¾¼ã¿"""
        
        default_benchmarks = {
            "small_scale": BenchmarkThresholds(
                max_time=10.0,
                max_memory=128.0,
                max_cpu_percent=70.0,
                max_memory_growth=50.0,
                max_gc_collections=5
            ),
            "medium_scale": BenchmarkThresholds(
                max_time=30.0,
                max_memory=256.0,
                max_cpu_percent=80.0,
                max_memory_growth=100.0,
                max_gc_collections=10
            ),
            "large_scale": BenchmarkThresholds(
                max_time=120.0,
                max_memory=512.0,
                max_cpu_percent=90.0,
                max_memory_growth=200.0,
                max_gc_collections=20
            ),
            "stress_test": BenchmarkThresholds(
                max_time=300.0,
                max_memory=1024.0,
                max_cpu_percent=95.0,
                max_memory_growth=500.0,
                max_gc_collections=50
            )
        }
        
        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                
                # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’æ›´æ–°
                for test_name, thresholds in config_data.get('benchmarks', {}).items():
                    default_benchmarks[test_name] = BenchmarkThresholds(**thresholds)
                    
                self.logger.info(f"Benchmarks loaded from: {config_file}")
                
            except Exception as e:
                self.logger.warning(f"Failed to load config file: {e}, using defaults")
        
        return default_benchmarks
        
    def validate_performance(self, test_name: str, metrics: PerformanceMetrics) -> ValidationResult:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼"""
        
        # å¯¾å¿œã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å–å¾—
        if test_name not in self.benchmarks:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã—ã¦ medium_scale ã‚’ä½¿ç”¨
            benchmark = self.benchmarks.get('medium_scale', self.benchmarks['small_scale'])
            self.logger.warning(f"No benchmark for {test_name}, using default")
        else:
            benchmark = self.benchmarks[test_name]
        
        violations = []
        
        # å„æŒ‡æ¨™ã‚’ãƒã‚§ãƒƒã‚¯
        if metrics.execution_time > benchmark.max_time:
            violations.append(f"å®Ÿè¡Œæ™‚é–“è¶…é: {metrics.execution_time:.2f}s > {benchmark.max_time}s")
            
        if metrics.peak_memory_mb > benchmark.max_memory:
            violations.append(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¶…é: {metrics.peak_memory_mb:.1f}MB > {benchmark.max_memory}MB")
            
        if metrics.avg_cpu_percent > benchmark.max_cpu_percent:
            violations.append(f"CPUä½¿ç”¨ç‡è¶…é: {metrics.avg_cpu_percent:.1f}% > {benchmark.max_cpu_percent}%")
            
        if metrics.memory_growth_mb > benchmark.max_memory_growth:
            violations.append(f"ãƒ¡ãƒ¢ãƒªå¢—åŠ é‡è¶…é: {metrics.memory_growth_mb:.1f}MB > {benchmark.max_memory_growth}MB")
            
        if metrics.gc_collections > benchmark.max_gc_collections:
            violations.append(f"GCå›æ•°è¶…é: {metrics.gc_collections} > {benchmark.max_gc_collections}")
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-100ï¼‰
        score = self._calculate_score(metrics, benchmark)
        
        # æ¤œè¨¼çµæœä½œæˆ
        result = ValidationResult(
            test_name=test_name,
            passed=len(violations) == 0,
            metrics=metrics,
            thresholds=benchmark,
            violations=violations,
            score=score
        )
        
        self.validation_history.append(result)
        
        # ãƒ­ã‚°å‡ºåŠ›
        status = "PASS" if result.passed else "FAIL"
        self.logger.info(f"Validation {status}: {test_name} (Score: {score:.1f}/100)")
        
        if violations:
            for violation in violations:
                self.logger.warning(f"  - {violation}")
        
        return result
        
    def _calculate_score(self, metrics: PerformanceMetrics, benchmark: BenchmarkThresholds) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-100ï¼‰"""
        
        # å„æŒ‡æ¨™ã®ã‚¹ã‚³ã‚¢ï¼ˆ0-25ãšã¤ã€åˆè¨ˆ100ï¼‰
        time_score = max(0, 25 * (1 - metrics.execution_time / benchmark.max_time))
        memory_score = max(0, 25 * (1 - metrics.peak_memory_mb / benchmark.max_memory))
        cpu_score = max(0, 25 * (1 - metrics.avg_cpu_percent / benchmark.max_cpu_percent))
        growth_score = max(0, 25 * (1 - metrics.memory_growth_mb / benchmark.max_memory_growth))
        
        total_score = time_score + memory_score + cpu_score + growth_score
        return min(100.0, max(0.0, total_score))
        
    def generate_performance_report(self, results: Optional[List[ValidationResult]] = None) -> str:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        if results is None:
            results = self.validation_history
            
        if not results:
            return "æ¤œè¨¼çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        report_lines = [
            "=" * 80,
            "5-3-3 è² è·ãƒ†ã‚¹ãƒˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ",
            "=" * 80,
            f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ•°: {len(results)}",
            ""
        ]
        
        # ã‚µãƒãƒªãƒ¼
        passed_count = sum(1 for r in results if r.passed)
        avg_score = sum(r.score for r in results) / len(results)
        
        report_lines.extend([
            "ğŸ“Š å®Ÿè¡Œã‚µãƒãƒªãƒ¼",
            "-" * 40,
            f"åˆæ ¼: {passed_count}/{len(results)} ({passed_count/len(results)*100:.1f}%)",
            f"å¹³å‡ã‚¹ã‚³ã‚¢: {avg_score:.1f}/100",
            ""
        ])
        
        # å€‹åˆ¥ãƒ†ã‚¹ãƒˆçµæœ
        report_lines.append("ğŸ“‹ å€‹åˆ¥ãƒ†ã‚¹ãƒˆçµæœ")
        report_lines.append("-" * 40)
        
        for result in results:
            status_emoji = "âœ…" if result.passed else "âŒ"
            report_lines.extend([
                f"{status_emoji} {result.test_name} (ã‚¹ã‚³ã‚¢: {result.score:.1f}/100)",
                f"   å®Ÿè¡Œæ™‚é–“: {result.metrics.execution_time:.2f}s",
                f"   ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {result.metrics.peak_memory_mb:.1f}MB", 
                f"   å¹³å‡CPU: {result.metrics.avg_cpu_percent:.1f}%",
                f"   ãƒ¡ãƒ¢ãƒªå¢—åŠ : {result.metrics.memory_growth_mb:.1f}MB",
                ""
            ])
            
            if result.violations:
                report_lines.append("   âš ï¸ é•åé …ç›®:")
                for violation in result.violations:
                    report_lines.append(f"     - {violation}")
                report_lines.append("")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‚¾å‘åˆ†æ
        if len(results) > 1:
            report_lines.extend([
                "ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‚¾å‘åˆ†æ",
                "-" * 40
            ])
            
            # å®Ÿè¡Œæ™‚é–“åˆ†æ
            exec_times = [r.metrics.execution_time for r in results]
            memory_peaks = [r.metrics.peak_memory_mb for r in results]
            
            report_lines.extend([
                f"å®Ÿè¡Œæ™‚é–“: æœ€å° {min(exec_times):.2f}s, æœ€å¤§ {max(exec_times):.2f}s, å¹³å‡ {sum(exec_times)/len(exec_times):.2f}s",
                f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨: æœ€å° {min(memory_peaks):.1f}MB, æœ€å¤§ {max(memory_peaks):.1f}MB, å¹³å‡ {sum(memory_peaks)/len(memory_peaks):.1f}MB",
                ""
            ])
        
        # æ¨å¥¨äº‹é …
        report_lines.extend([
            "ğŸ’¡ æ¨å¥¨äº‹é …",
            "-" * 40
        ])
        
        failed_results = [r for r in results if not r.passed]
        if failed_results:
            report_lines.append("â— æ”¹å–„ãŒå¿…è¦ãªé …ç›®:")
            common_violations = {}
            for result in failed_results:
                for violation in result.violations:
                    violation_type = violation.split(':')[0]
                    common_violations[violation_type] = common_violations.get(violation_type, 0) + 1
                    
            for violation_type, count in sorted(common_violations.items(), key=lambda x: x[1], reverse=True):
                report_lines.append(f"  - {violation_type}: {count}ä»¶")
                
            report_lines.extend([
                "",
                "ğŸ”§ å¯¾ç­–æ¡ˆ:",
                "  - ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æœ€é©åŒ–",
                "  - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å‰Šæ¸›",
                "  - ä¸¦åˆ—å‡¦ç†ã®æ”¹å–„",
                "  - ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿæ§‹ã®å°å…¥",
            ])
        else:
            report_lines.append("âœ¨ å…¨ãƒ†ã‚¹ãƒˆãŒåŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã™ï¼")
        
        report_lines.append("")
        report_lines.append("=" * 80)
        
        return "\n".join(report_lines)
        
    def export_results(self, filepath: str, results: Optional[List[ValidationResult]] = None):
        """çµæœã‚’JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        
        if results is None:
            results = self.validation_history
            
        try:
            export_data = {
                "report_info": {
                    "generated_at": datetime.now().isoformat(),
                    "total_tests": len(results),
                    "passed_tests": sum(1 for r in results if r.passed),
                    "average_score": sum(r.score for r in results) / len(results) if results else 0
                },
                "test_results": [
                    {
                        "test_name": r.test_name,
                        "passed": r.passed,
                        "score": r.score,
                        "violations": r.violations,
                        "metrics": {
                            "execution_time": r.metrics.execution_time,
                            "peak_memory_mb": r.metrics.peak_memory_mb,
                            "avg_cpu_percent": r.metrics.avg_cpu_percent,
                            "memory_growth_mb": r.metrics.memory_growth_mb,
                            "gc_collections": r.metrics.gc_collections,
                            "start_memory_mb": r.metrics.start_memory_mb,
                            "end_memory_mb": r.metrics.end_memory_mb,
                            "thread_count": r.metrics.thread_count,
                            "process_count": r.metrics.process_count
                        },
                        "thresholds": {
                            "max_time": r.thresholds.max_time,
                            "max_memory": r.thresholds.max_memory,
                            "max_cpu_percent": r.thresholds.max_cpu_percent,
                            "max_memory_growth": r.thresholds.max_memory_growth,
                            "max_gc_collections": r.thresholds.max_gc_collections
                        }
                    }
                    for r in results
                ],
                "benchmarks": {
                    name: {
                        "max_time": bench.max_time,
                        "max_memory": bench.max_memory,
                        "max_cpu_percent": bench.max_cpu_percent,
                        "max_memory_growth": bench.max_memory_growth,
                        "max_gc_collections": bench.max_gc_collections
                    }
                    for name, bench in self.benchmarks.items()
                }
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
                
            self.logger.info(f"Results exported to: {filepath}")
            
        except Exception as e:
            self.logger.error(f"Export error: {e}")
            
    def clear_history(self):
        """æ¤œè¨¼å±¥æ­´ã‚¯ãƒªã‚¢"""
        self.validation_history.clear()
        self.logger.info("Validation history cleared")

# ãƒ†ã‚¹ãƒˆç”¨ã®å®Ÿè¡Œä¾‹
if __name__ == "__main__":
    import time
    
    # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
    logging.basicConfig(level=logging.INFO)
    
    validator = BenchmarkValidator()
    
    # æ¨¡æ“¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½œæˆ
    test_metrics = PerformanceMetrics(
        execution_time=5.2,
        peak_memory_mb=89.5,
        avg_cpu_percent=45.2,
        memory_growth_mb=25.3,
        gc_collections=3,
        start_memory_mb=64.2,
        end_memory_mb=89.5,
        thread_count=4,
        process_count=120
    )
    
    # æ¤œè¨¼å®Ÿè¡Œ
    result = validator.validate_performance("small_scale", test_metrics)
    
    print(f"æ¤œè¨¼çµæœ: {'åˆæ ¼' if result.passed else 'ä¸åˆæ ¼'}")
    print(f"ã‚¹ã‚³ã‚¢: {result.score:.1f}/100")
    
    if result.violations:
        print("é•åé …ç›®:")
        for violation in result.violations:
            print(f"  - {violation}")
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\n" + "="*50)
    print(validator.generate_performance_report())
