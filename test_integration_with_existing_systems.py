"""
Integration Test: Metric Normalization with Existing Systems
File: test_integration_with_existing_systems.py
Description: 
  2-1-3ã€ŒæŒ‡æ¨™ã®æ­£è¦åŒ–æ‰‹æ³•ã®è¨­è¨ˆã€ã¨æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆãƒ†ã‚¹ãƒˆ
  2-1-1ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã€2-1-2æŒ‡æ¨™é¸å®šã‚·ã‚¹ãƒ†ãƒ ã¨ã®é€£æºç¢ºèª

Author: imega
Created: 2025-07-10
Modified: 2025-07-10
"""

import logging
import numpy as np
import pandas as pd
from pathlib import Path

# æ­£è¦åŒ–ã‚·ã‚¹ãƒ†ãƒ 
from config.metric_normalization_manager import MetricNormalizationManager

# æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ 
try:
    from config.strategy_scoring_model import StrategyScoreManager
    from config.metric_selection_manager import MetricSelectionManager
    EXISTING_SYSTEMS_AVAILABLE = True
except ImportError as e:
    EXISTING_SYSTEMS_AVAILABLE = False
    print(f"Warning: Some existing systems not available: {e}")

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

def create_realistic_strategy_data():
    """ãƒªã‚¢ãƒ«ãªæˆ¦ç•¥ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"""
    np.random.seed(42)
    
    strategies = {
        "momentum_strategy": {
            "sharpe_ratio": np.random.normal(1.5, 0.8, 252),  # 1å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿
            "sortino_ratio": np.random.normal(2.0, 1.0, 252),
            "profit_factor": np.random.exponential(2.5, 252),
            "win_rate": np.random.beta(3, 2, 252),
            "max_drawdown": np.random.exponential(0.08, 252) * -1,
            "total_return": np.cumsum(np.random.normal(0.0005, 0.02, 252))
        },
        "mean_reversion_strategy": {
            "sharpe_ratio": np.random.normal(1.2, 0.6, 252),
            "sortino_ratio": np.random.normal(1.6, 0.8, 252),
            "profit_factor": np.random.exponential(1.8, 252),
            "win_rate": np.random.beta(2, 2, 252),
            "max_drawdown": np.random.exponential(0.12, 252) * -1,
            "total_return": np.cumsum(np.random.normal(0.0003, 0.015, 252))
        },
        "statistical_arbitrage": {
            "sharpe_ratio": np.random.normal(2.2, 0.4, 252),
            "sortino_ratio": np.random.normal(2.8, 0.5, 252),
            "profit_factor": np.random.exponential(3.5, 252),
            "win_rate": np.random.beta(4, 1, 252),
            "max_drawdown": np.random.exponential(0.04, 252) * -1,
            "total_return": np.cumsum(np.random.normal(0.0002, 0.008, 252))
        }
    }
    
    return strategies

def test_standalone_normalization():
    """å˜ç‹¬æ­£è¦åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== Testing Standalone Normalization ===")
    
    try:
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        strategies_data = create_realistic_strategy_data()
        
        # æ­£è¦åŒ–ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ä½œæˆï¼ˆstandalone modeï¼‰
        manager = MetricNormalizationManager(integration_mode="standalone")
        
        # ä¸€æ‹¬æ­£è¦åŒ–å®Ÿè¡Œ
        summaries = manager.batch_normalize_strategies(strategies_data, save_sessions=True)
        
        # çµæœæ¤œè¨¼
        total_strategies = len(summaries)
        successful = sum(1 for s in summaries.values() if s.success)
        
        logger.info(f"âœ“ Standalone normalization completed")
        logger.info(f"  - Total strategies: {total_strategies}")
        logger.info(f"  - Successful: {successful}")
        logger.info(f"  - Success rate: {successful/total_strategies:.1%}")
        
        # å„æˆ¦ç•¥ã®è©³ç´°çµæœ
        for strategy_name, summary in summaries.items():
            metrics_count = len(summary.session_info.metrics_processed)
            logger.info(f"  - {strategy_name}: {metrics_count} metrics, success: {summary.success}")
        
        return successful == total_strategies
        
    except Exception as e:
        logger.error(f"âœ— Standalone normalization failed: {e}")
        return False

def test_scoring_system_integration():
    """ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ"""
    logger.info("=== Testing Scoring System Integration ===")
    
    if not EXISTING_SYSTEMS_AVAILABLE:
        logger.warning("Existing systems not available, skipping test")
        return True
    
    try:
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        strategies_data = create_realistic_strategy_data()
        
        # æ­£è¦åŒ–ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ï¼ˆscoring modeï¼‰
        normalization_manager = MetricNormalizationManager(integration_mode="scoring")
        
        # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        scoring_manager = StrategyScoreManager()
        
        # æ­£è¦åŒ–å®Ÿè¡Œ
        summaries = normalization_manager.batch_normalize_strategies(strategies_data)
        
        # æ­£è¦åŒ–çµæœã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã§æ´»ç”¨ã™ã‚‹ä¾‹
        integration_results = {}
        
        for strategy_name, summary in summaries.items():
            if summary.success and summary.scoring_integration:
                # æ­£è¦åŒ–ã•ã‚ŒãŸå€¤ã‚’å–å¾—
                normalized_values = summary.scoring_integration.get("normalized_values", {})
                
                # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã§ã®å‡¦ç†ï¼ˆä¾‹ï¼šé‡ã¿ä»˜ãã‚¹ã‚³ã‚¢è¨ˆç®—ï¼‰
                if normalized_values:
                    # ã‚·ãƒ³ãƒ—ãƒ«ãªé‡ã¿ä»˜ãã‚¹ã‚³ã‚¢è¨ˆç®—
                    weighted_score = (
                        normalized_values.get("sharpe_ratio", 0) * 0.3 +
                        normalized_values.get("profit_factor", 0) * 0.25 +
                        normalized_values.get("win_rate", 0) * 0.2 +
                        abs(normalized_values.get("max_drawdown", 0)) * 0.15 +  # ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ã¯çµ¶å¯¾å€¤
                        normalized_values.get("total_return", 0) * 0.1
                    )
                    
                    integration_results[strategy_name] = {
                        "weighted_score": weighted_score,
                        "normalization_success": True,
                        "metrics_count": len(normalized_values)
                    }
                    
                    logger.info(f"  - {strategy_name}: normalized score = {weighted_score:.3f}")
        
        success_count = len(integration_results)
        logger.info(f"âœ“ Scoring integration completed: {success_count} strategies processed")
        
        return success_count > 0
        
    except Exception as e:
        logger.error(f"âœ— Scoring system integration failed: {e}")
        return False

def test_metric_selection_integration():
    """æŒ‡æ¨™é¸å®šã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ"""
    logger.info("=== Testing Metric Selection Integration ===")
    
    if not EXISTING_SYSTEMS_AVAILABLE:
        logger.warning("Existing systems not available, skipping test")
        return True
    
    try:
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        strategies_data = create_realistic_strategy_data()
        
        # æ­£è¦åŒ–ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ï¼ˆmetric_selection modeï¼‰
        normalization_manager = MetricNormalizationManager(integration_mode="metric_selection")
        
        # æŒ‡æ¨™é¸å®šãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        selection_manager = MetricSelectionManager()
        
        # Step 1: æ­£è¦åŒ–å®Ÿè¡Œ
        normalization_summaries = normalization_manager.batch_normalize_strategies(strategies_data)
        
        # Step 2: æ­£è¦åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡æ¨™é¸å®šã‚·ã‚¹ãƒ†ãƒ ã§ä½¿ç”¨
        for strategy_name, summary in normalization_summaries.items():
            if summary.success and summary.selection_integration:
                selection_data = summary.selection_integration.get("data_summary", {})
                
                if selection_data:
                    # æ­£è¦åŒ–æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡æ¨™é¸å®šã‚·ã‚¹ãƒ†ãƒ ç”¨ã«å¤‰æ›
                    metrics_for_selection = {}
                    for metric_name, data_info in selection_data.items():
                        normalized_values = data_info.get("normalized_values", [])
                        if normalized_values:
                            metrics_for_selection[metric_name] = np.array(normalized_values)
                    
                    if metrics_for_selection:
                        # æŒ‡æ¨™é¸å®šã‚·ã‚¹ãƒ†ãƒ ã§ã®å‰å‡¦ç†ã¨ã—ã¦æ­£è¦åŒ–ã‚’ä½¿ç”¨
                        logger.info(f"  - {strategy_name}: {len(metrics_for_selection)} normalized metrics ready for selection")
        
        logger.info("âœ“ Metric selection integration completed")
        return True
        
    except Exception as e:
        logger.error(f"âœ— Metric selection integration failed: {e}")
        return False

def test_end_to_end_workflow():
    """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== Testing End-to-End Workflow ===")
    
    try:
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        strategies_data = create_realistic_strategy_data()
        
        # Step 1: æ­£è¦åŒ–
        normalization_manager = MetricNormalizationManager(integration_mode="metric_selection")
        normalization_summaries = normalization_manager.batch_normalize_strategies(strategies_data)
        
        # Step 2: æ­£è¦åŒ–çµæœã®æ¤œè¨¼
        normalized_strategies = {}
        for strategy_name, summary in normalization_summaries.items():
            if summary.success:
                # æ­£è¦åŒ–å¾Œã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
                session_results = summary.session_info.results
                normalized_metrics = {}
                
                for metric_name, result_info in session_results.items():
                    if result_info.get("success", False):
                        # æ­£è¦åŒ–ã•ã‚ŒãŸçµ±è¨ˆæƒ…å ±ã‚’ä½¿ç”¨
                        stats = result_info.get("statistics", {})
                        normalized_mean = stats.get("normalized_mean", 0)
                        normalized_metrics[metric_name] = normalized_mean
                
                normalized_strategies[strategy_name] = normalized_metrics
        
        # Step 3: æ­£è¦åŒ–å“è³ªã®è©•ä¾¡
        quality_scores = {}
        for strategy_name, summary in normalization_summaries.items():
            if summary.success:
                # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã«åŸºã¥ãå“è³ªè©•ä¾¡
                confidence_scores = []
                session_results = summary.session_info.results
                
                for result_info in session_results.values():
                    confidence = result_info.get("confidence_score", 0)
                    confidence_scores.append(confidence)
                
                avg_confidence = np.mean(confidence_scores) if confidence_scores else 0
                quality_scores[strategy_name] = avg_confidence
                
                logger.info(f"  - {strategy_name}: quality score = {avg_confidence:.3f}")
        
        # Step 4: åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        comprehensive_report = normalization_manager.generate_comprehensive_report(
            normalization_summaries, save_report=True
        )
        
        # Step 5: çµæœã‚µãƒãƒªãƒ¼
        total_strategies = len(strategies_data)
        normalized_count = len(normalized_strategies)
        avg_quality = np.mean(list(quality_scores.values())) if quality_scores else 0
        
        logger.info(f"âœ“ End-to-end workflow completed")
        logger.info(f"  - Input strategies: {total_strategies}")
        logger.info(f"  - Successfully normalized: {normalized_count}")
        logger.info(f"  - Average quality score: {avg_quality:.3f}")
        logger.info(f"  - Overall success rate: {normalized_count/total_strategies:.1%}")
        
        return normalized_count == total_strategies and avg_quality > 0.7
        
    except Exception as e:
        logger.error(f"âœ— End-to-end workflow failed: {e}")
        return False

def test_performance_with_large_dataset():
    """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== Testing Performance with Large Dataset ===")
    
    try:
        import time
        
        # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
        large_strategies = {}
        num_strategies = 20
        data_points_per_metric = 5000  # ç´„20å¹´åˆ†ã®æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿
        
        logger.info(f"Creating large dataset: {num_strategies} strategies x 6 metrics x {data_points_per_metric} points")
        
        for i in range(num_strategies):
            strategy_name = f"large_strategy_{i:02d}"
            large_strategies[strategy_name] = {
                "sharpe_ratio": np.random.normal(1.0 + i*0.1, 0.5, data_points_per_metric),
                "sortino_ratio": np.random.normal(1.3 + i*0.1, 0.6, data_points_per_metric),
                "profit_factor": np.random.exponential(1.5 + i*0.2, data_points_per_metric),
                "win_rate": np.random.beta(2 + i*0.1, 2, data_points_per_metric),
                "max_drawdown": np.random.exponential(0.1 + i*0.01, data_points_per_metric) * -1,
                "total_return": np.cumsum(np.random.normal(0.0003 + i*0.0001, 0.02, data_points_per_metric))
            }
        
        # æ­£è¦åŒ–å®Ÿè¡Œï¼ˆæ™‚é–“æ¸¬å®šï¼‰
        normalization_manager = MetricNormalizationManager(integration_mode="standalone")
        
        start_time = time.time()
        summaries = normalization_manager.batch_normalize_strategies(large_strategies, save_sessions=False)
        processing_time = time.time() - start_time
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®è¨ˆç®—
        total_data_points = num_strategies * 6 * data_points_per_metric
        successful_strategies = sum(1 for s in summaries.values() if s.success)
        throughput = total_data_points / processing_time
        
        logger.info(f"âœ“ Large dataset performance test completed")
        logger.info(f"  - Total data points: {total_data_points:,}")
        logger.info(f"  - Processing time: {processing_time:.2f} seconds")
        logger.info(f"  - Throughput: {throughput:.0f} data points/second")
        logger.info(f"  - Successful strategies: {successful_strategies}/{num_strategies}")
        logger.info(f"  - Memory efficiency: OK (no memory errors)")
        
        return successful_strategies == num_strategies and throughput > 1000
        
    except Exception as e:
        logger.error(f"âœ— Large dataset performance test failed: {e}")
        return False

def generate_integration_report(test_results):
    """çµ±åˆãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
    logger.info("=== Integration Test Results Summary ===")
    
    total_tests = len(test_results)
    passed_tests = sum(test_results.values())
    
    logger.info(f"Total Integration Tests: {total_tests}")
    logger.info(f"Passed: {passed_tests}")
    logger.info(f"Failed: {total_tests - passed_tests}")
    logger.info(f"Success Rate: {passed_tests/total_tests*100:.1f}%")
    
    logger.info("\nDetailed Results:")
    for test_name, result in test_results.items():
        status = "âœ“ PASS" if result else "âœ— FAIL"
        logger.info(f"  {test_name}: {status}")
    
    # ã‚·ã‚¹ãƒ†ãƒ çµ±åˆçŠ¶æ³ã®è©•ä¾¡
    integration_quality = "EXCELLENT" if passed_tests == total_tests else \
                         "GOOD" if passed_tests >= total_tests * 0.8 else \
                         "NEEDS_IMPROVEMENT"
    
    logger.info(f"\nIntegration Quality: {integration_quality}")
    
    if passed_tests == total_tests:
        logger.info("\nğŸ‰ All integration tests passed! 2-1-3 Normalization System is fully integrated.")
    else:
        logger.warning("\nâš ï¸ Some integration tests failed. Please review and fix issues.")

def main():
    """ãƒ¡ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    logger.info("Starting Integration Test for 2-1-3 Metric Normalization System")
    logger.info("="*70)
    
    # çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®å®Ÿè¡Œ
    test_results = {
        "Standalone Normalization": test_standalone_normalization(),
        "Scoring System Integration": test_scoring_system_integration(),
        "Metric Selection Integration": test_metric_selection_integration(),
        "End-to-End Workflow": test_end_to_end_workflow(),
        "Large Dataset Performance": test_performance_with_large_dataset()
    }
    
    # çµæœãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
    generate_integration_report(test_results)

if __name__ == "__main__":
    main()
