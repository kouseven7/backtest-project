"""
Module: Concurrent Execution Scheduler
File: concurrent_execution_scheduler.py
Description: 
  4-1-3ã€Œãƒãƒ«ãƒæˆ¦ç•¥åŒæ™‚å®Ÿè¡Œã®èª¿æ•´æ©Ÿèƒ½ã€
  ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ä¸¦è¡Œå®Ÿè¡Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼

Author: imega
Created: 2025-07-20
Modified: 2025-07-20

Functions:
  - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ç®¡ç† (Thread/Process/Async)
  - å‹•çš„è² è·åˆ†æ•£ãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿè¡Œåˆ¶å¾¡
  - å®Ÿè¡ŒçŠ¶æ…‹ç›£è¦–ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
  - ä¾‹å¤–å‡¦ç†ãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
"""

import os
import sys
import json
import time
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable, Tuple, Union
from enum import Enum
from dataclasses import dataclass, asdict, field
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, Future, as_completed
import threading
import multiprocessing
from functools import wraps
import traceback
import signal

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from resource_allocation_engine import ResourceAllocation, ExecutionMode, Priority, ResourceAllocationEngine
    from strategy_dependency_resolver import DependencyResolution, ExecutionGraph, StrategyDependencyResolver
except ImportError:
    # ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ãƒ†ã‚¹ãƒˆç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    logger = logging.getLogger(__name__)
    logger.warning("Could not import project modules, using fallback definitions")
    
    class ExecutionMode(Enum):
        THREAD = "thread"
        PROCESS = "process"
        ASYNC = "async"
        AUTO = "auto"

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger(__name__)

class ExecutionStatus(Enum):
    """å®Ÿè¡ŒçŠ¶æ…‹"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    TIMEOUT = "timeout"

class SchedulerState(Enum):
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼çŠ¶æ…‹"""
    IDLE = "idle"
    SCHEDULING = "scheduling"
    EXECUTING = "executing"
    PAUSED = "paused"
    STOPPED = "stopped"

@dataclass
class ExecutionTask:
    """å®Ÿè¡Œã‚¿ã‚¹ã‚¯"""
    task_id: str
    strategy_name: str
    execution_mode: ExecutionMode
    function: Callable
    args: Tuple[Any, ...] = field(default_factory=tuple)
    kwargs: Dict[str, Any] = field(default_factory=dict)
    priority: int = 1
    timeout: float = 300.0
    retry_count: int = 3
    dependencies: List[str] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        result['execution_mode'] = self.execution_mode.value
        result['created_at'] = self.created_at.isoformat()
        # é–¢æ•°ã¯æ–‡å­—åˆ—è¡¨ç¾ã«å¤‰æ›
        result['function'] = str(self.function)
        return result

@dataclass
class ExecutionResult:
    """å®Ÿè¡Œçµæœ"""
    task_id: str
    strategy_name: str
    status: ExecutionStatus
    result: Any = None
    error: Optional[str] = None
    execution_time: float = 0.0
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    retry_count: int = 0
    resource_usage: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        result['status'] = self.status.value
        result['start_time'] = self.start_time.isoformat() if self.start_time else None
        result['end_time'] = self.end_time.isoformat() if self.end_time else None
        return result

@dataclass
class SchedulingPlan:
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°è¨ˆç”»"""
    execution_batches: List[List[ExecutionTask]] = field(default_factory=list)
    total_estimated_time: float = 0.0
    parallel_factor: float = 1.0
    resource_requirements: Dict[str, Any] = field(default_factory=dict)
    optimization_notes: List[str] = field(default_factory=list)

class TaskExecutor:
    """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œå™¨åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, execution_mode: ExecutionMode, config: Dict[str, Any]):
        self.execution_mode = execution_mode
        self.config = config
        self.active_tasks: Dict[str, Future] = {}
        self.task_lock = threading.Lock()
    
    def submit_task(self, task: ExecutionTask) -> Future:
        """ã‚¿ã‚¹ã‚¯æŠ•å…¥"""
        raise NotImplementedError
    
    def cancel_task(self, task_id: str) -> bool:
        """ã‚¿ã‚¹ã‚¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«"""
        with self.task_lock:
            if task_id in self.active_tasks:
                future = self.active_tasks[task_id]
                success = future.cancel()
                if success:
                    del self.active_tasks[task_id]
                return success
            return False
    
    def get_active_task_count(self) -> int:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¿ã‚¹ã‚¯æ•°å–å¾—"""
        with self.task_lock:
            return len(self.active_tasks)
    
    def shutdown(self):
        """ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
        with self.task_lock:
            for future in self.active_tasks.values():
                future.cancel()
            self.active_tasks.clear()

class ThreadTaskExecutor(TaskExecutor):
    """ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¹ã‚¯å®Ÿè¡Œå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(ExecutionMode.THREAD, config)
        max_workers = config.get('execution_modes', {}).get('thread_pool_size', 4)
        self.executor = ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="ConcurrentScheduler")
        logger.info(f"Initialized ThreadTaskExecutor with {max_workers} workers")
    
    def submit_task(self, task: ExecutionTask) -> Future:
        """ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¹ã‚¯æŠ•å…¥"""
        wrapped_function = self._wrap_function_with_monitoring(task)
        future = self.executor.submit(wrapped_function, *task.args, **task.kwargs)
        
        with self.task_lock:
            self.active_tasks[task.task_id] = future
        
        # ã‚¿ã‚¹ã‚¯å®Œäº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        future.add_done_callback(lambda f: self._cleanup_task(task.task_id))
        
        return future
    
    def _wrap_function_with_monitoring(self, task: ExecutionTask) -> Callable:
        """é–¢æ•°ã‚’ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ä»˜ãã§ãƒ©ãƒƒãƒ—"""
        @wraps(task.function)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                logger.debug(f"Thread execution started for {task.strategy_name} ({task.task_id})")
                result = task.function(*args, **kwargs)
                execution_time = time.time() - start_time
                logger.debug(f"Thread execution completed for {task.strategy_name} in {execution_time:.2f}s")
                return result
            except Exception as e:
                execution_time = time.time() - start_time
                logger.error(f"Thread execution failed for {task.strategy_name} after {execution_time:.2f}s: {e}")
                raise
        
        return wrapper
    
    def _cleanup_task(self, task_id: str):
        """ã‚¿ã‚¹ã‚¯ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        with self.task_lock:
            self.active_tasks.pop(task_id, None)
    
    def shutdown(self):
        """ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
        super().shutdown()
        self.executor.shutdown(wait=True)
        logger.info("ThreadTaskExecutor shutdown complete")

class ProcessTaskExecutor(TaskExecutor):
    """ãƒ—ãƒ­ã‚»ã‚¹ã‚¿ã‚¹ã‚¯å®Ÿè¡Œå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(ExecutionMode.PROCESS, config)
        max_workers = config.get('execution_modes', {}).get('process_pool_size', 2)
        self.executor = ProcessPoolExecutor(max_workers=max_workers)
        logger.info(f"Initialized ProcessTaskExecutor with {max_workers} workers")
    
    def submit_task(self, task: ExecutionTask) -> Future:
        """ãƒ—ãƒ­ã‚»ã‚¹ã‚¿ã‚¹ã‚¯æŠ•å…¥"""
        # ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œç”¨ã«é–¢æ•°ã‚’ãƒ©ãƒƒãƒ—
        wrapped_function = self._create_process_wrapper(task)
        future = self.executor.submit(wrapped_function, task.strategy_name, *task.args, **task.kwargs)
        
        with self.task_lock:
            self.active_tasks[task.task_id] = future
        
        # ã‚¿ã‚¹ã‚¯å®Œäº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        future.add_done_callback(lambda f: self._cleanup_task(task.task_id))
        
        return future
    
    def _create_process_wrapper(self, task: ExecutionTask) -> Callable:
        """ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œç”¨ãƒ©ãƒƒãƒ‘ãƒ¼ä½œæˆ"""
        def process_wrapper(strategy_name: str, *args, **kwargs):
            """ãƒ—ãƒ­ã‚»ã‚¹å†…ã§å®Ÿè¡Œã•ã‚Œã‚‹é–¢æ•°ãƒ©ãƒƒãƒ‘ãƒ¼"""
            import time
            start_time = time.time()
            try:
                print(f"Process execution started for {strategy_name}")
                result = task.function(*args, **kwargs)
                execution_time = time.time() - start_time
                print(f"Process execution completed for {strategy_name} in {execution_time:.2f}s")
                return result
            except Exception as e:
                execution_time = time.time() - start_time
                print(f"Process execution failed for {strategy_name} after {execution_time:.2f}s: {e}")
                raise
        
        return process_wrapper
    
    def _cleanup_task(self, task_id: str):
        """ã‚¿ã‚¹ã‚¯ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        with self.task_lock:
            self.active_tasks.pop(task_id, None)
    
    def shutdown(self):
        """ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
        super().shutdown()
        self.executor.shutdown(wait=True)
        logger.info("ProcessTaskExecutor shutdown complete")

class AsyncTaskExecutor(TaskExecutor):
    """éåŒæœŸã‚¿ã‚¹ã‚¯å®Ÿè¡Œå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(ExecutionMode.ASYNC, config)
        self.loop: Optional[asyncio.AbstractEventLoop] = None
        self.async_tasks: Dict[str, asyncio.Task] = {}
        self.running = False
        logger.info("Initialized AsyncTaskExecutor")
    
    def submit_task(self, task: ExecutionTask) -> Future:
        """éåŒæœŸã‚¿ã‚¹ã‚¯æŠ•å…¥"""
        if not self.loop or self.loop.is_closed():
            self.loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self.loop)
        
        # åŒæœŸé–¢æ•°ã‚’éåŒæœŸã§ãƒ©ãƒƒãƒ—
        async_wrapper = self._create_async_wrapper(task)
        
        # Futureä½œæˆï¼ˆThreadPoolExecutorã®Futureã¨äº’æ›æ€§ã‚’æŒãŸã›ã‚‹ï¼‰
        future = Future()
        
        # éåŒæœŸã‚¿ã‚¹ã‚¯ä½œæˆã¨å®Ÿè¡Œ
        async_task = self.loop.create_task(async_wrapper())
        
        with self.task_lock:
            self.active_tasks[task.task_id] = future
            self.async_tasks[task.task_id] = async_task
        
        # çµæœã‚’Futureã«è¨­å®šã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        def set_future_result(async_task: asyncio.Task):
            try:
                if async_task.cancelled():
                    future.cancel()
                elif async_task.exception():
                    future.set_exception(async_task.exception())
                else:
                    future.set_result(async_task.result())
            except Exception as e:
                future.set_exception(e)
            
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self._cleanup_task(task.task_id)
        
        async_task.add_done_callback(set_future_result)
        
        return future
    
    def _create_async_wrapper(self, task: ExecutionTask) -> Callable:
        """éåŒæœŸå®Ÿè¡Œç”¨ãƒ©ãƒƒãƒ‘ãƒ¼ä½œæˆ"""
        async def async_wrapper():
            start_time = time.time()
            try:
                logger.debug(f"Async execution started for {task.strategy_name} ({task.task_id})")
                
                # åŒæœŸé–¢æ•°ã‚’éåŒæœŸã§å®Ÿè¡Œ
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, task.function, *task.args)
                
                execution_time = time.time() - start_time
                logger.debug(f"Async execution completed for {task.strategy_name} in {execution_time:.2f}s")
                return result
            except Exception as e:
                execution_time = time.time() - start_time
                logger.error(f"Async execution failed for {task.strategy_name} after {execution_time:.2f}s: {e}")
                raise
        
        return async_wrapper
    
    def _cleanup_task(self, task_id: str):
        """ã‚¿ã‚¹ã‚¯ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        with self.task_lock:
            self.active_tasks.pop(task_id, None)
            self.async_tasks.pop(task_id, None)
    
    def cancel_task(self, task_id: str) -> bool:
        """éåŒæœŸã‚¿ã‚¹ã‚¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«"""
        with self.task_lock:
            if task_id in self.async_tasks:
                async_task = self.async_tasks[task_id]
                success = async_task.cancel()
                if success:
                    del self.async_tasks[task_id]
                    self.active_tasks.pop(task_id, None)
                return success
            return super().cancel_task(task_id)
    
    def shutdown(self):
        """ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
        with self.task_lock:
            for async_task in self.async_tasks.values():
                async_task.cancel()
            self.async_tasks.clear()
        
        super().shutdown()
        
        if self.loop and not self.loop.is_closed():
            self.loop.close()
        
        logger.info("AsyncTaskExecutor shutdown complete")

class ConcurrentExecutionScheduler:
    """ä¸¦è¡Œå®Ÿè¡Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼"""
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆæœŸåŒ–"""
        self.config = self._load_config(config_path)
        self.state = SchedulerState.IDLE
        self.executors: Dict[ExecutionMode, TaskExecutor] = {}
        self.pending_tasks: List[ExecutionTask] = []
        self.running_tasks: Dict[str, ExecutionTask] = {}
        self.completed_tasks: Dict[str, ExecutionResult] = {}
        self.execution_history: List[Dict[str, Any]] = []
        self.task_counter = 0
        self.scheduler_lock = threading.Lock()
        
        # å®Ÿè¡Œå™¨åˆæœŸåŒ–
        self._initialize_executors()
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ç”¨
        self.performance_metrics = {
            'total_tasks_submitted': 0,
            'total_tasks_completed': 0,
            'total_tasks_failed': 0,
            'average_execution_time': 0.0,
            'peak_concurrent_tasks': 0
        }
        
    def _load_config(self, config_path: Optional[str] = None) -> Dict[str, Any]:
        """è¨­å®šèª­ã¿è¾¼ã¿"""
        if config_path is None:
            config_path = os.path.join(os.path.dirname(__file__), 'coordination_config.json')
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"Failed to load config from {config_path}: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š"""
        return {
            "execution_modes": {
                "thread_pool_size": 4,
                "process_pool_size": 2,
                "execution_timeout": 300,
                "default_retry_count": 3
            },
            "scheduling": {
                "max_concurrent_tasks": 10,
                "task_submission_interval": 0.1,
                "load_balancing_interval": 5.0
            }
        }
    
    def _initialize_executors(self):
        """å®Ÿè¡Œå™¨åˆæœŸåŒ–"""
        try:
            self.executors[ExecutionMode.THREAD] = ThreadTaskExecutor(self.config)
            self.executors[ExecutionMode.PROCESS] = ProcessTaskExecutor(self.config)
            self.executors[ExecutionMode.ASYNC] = AsyncTaskExecutor(self.config)
            logger.info("All task executors initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize executors: {e}")
            raise
    
    def create_scheduling_plan(self, tasks: List[ExecutionTask]) -> SchedulingPlan:
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°è¨ˆç”»ä½œæˆ"""
        logger.info(f"Creating scheduling plan for {len(tasks)} tasks")
        
        # ä¾å­˜é–¢ä¿‚ã«ã‚ˆã‚‹å®Ÿè¡Œé †åºæ±ºå®š
        execution_batches = self._group_tasks_by_dependencies(tasks)
        
        # ãƒªã‚½ãƒ¼ã‚¹è¦æ±‚é‡è¨ˆç®—
        resource_requirements = self._calculate_resource_requirements(tasks)
        
        # æ¨å®šå®Ÿè¡Œæ™‚é–“è¨ˆç®—
        total_estimated_time = self._calculate_total_execution_time(execution_batches)
        
        # ä¸¦åˆ—åº¦è¨ˆç®—
        parallel_factor = self._calculate_parallel_factor(execution_batches)
        
        # æœ€é©åŒ–ææ¡ˆ
        optimization_notes = self._generate_optimization_notes(execution_batches, resource_requirements)
        
        plan = SchedulingPlan(
            execution_batches=execution_batches,
            total_estimated_time=total_estimated_time,
            parallel_factor=parallel_factor,
            resource_requirements=resource_requirements,
            optimization_notes=optimization_notes
        )
        
        logger.info(f"Scheduling plan created: {len(execution_batches)} batches, {total_estimated_time:.1f}s estimated time")
        return plan
    
    def _group_tasks_by_dependencies(self, tasks: List[ExecutionTask]) -> List[List[ExecutionTask]]:
        """ä¾å­˜é–¢ä¿‚ã«ã‚ˆã‚‹ã‚¿ã‚¹ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—åŒ–"""
        batches = []
        remaining_tasks = tasks.copy()
        
        while remaining_tasks:
            # ç¾åœ¨å®Ÿè¡Œå¯èƒ½ãªã‚¿ã‚¹ã‚¯ï¼ˆä¾å­˜é–¢ä¿‚ãŒè§£æ±ºæ¸ˆã¿ï¼‰ã‚’è¦‹ã¤ã‘ã‚‹
            ready_tasks = []
            for task in remaining_tasks:
                if not task.dependencies or all(
                    dep_id in [t.task_id for batch in batches for t in batch] 
                    for dep_id in task.dependencies
                ):
                    ready_tasks.append(task)
            
            if not ready_tasks:
                # ä¾å­˜é–¢ä¿‚ãŒè§£æ±ºã§ããªã„å ´åˆã€æ®‹ã‚Šã®ã‚¿ã‚¹ã‚¯ã‚’å¼·åˆ¶å®Ÿè¡Œ
                logger.warning("Unresolved dependencies found, forcing execution of remaining tasks")
                ready_tasks = remaining_tasks.copy()
            
            # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
            ready_tasks.sort(key=lambda t: t.priority, reverse=True)
            
            batches.append(ready_tasks)
            
            # å‡¦ç†æ¸ˆã¿ã‚¿ã‚¹ã‚¯ã‚’é™¤å»
            for task in ready_tasks:
                remaining_tasks.remove(task)
        
        return batches
    
    def _calculate_resource_requirements(self, tasks: List[ExecutionTask]) -> Dict[str, Any]:
        """ãƒªã‚½ãƒ¼ã‚¹è¦æ±‚é‡è¨ˆç®—"""
        total_cpu = 0.0
        total_memory = 0.0
        execution_mode_count = {mode: 0 for mode in ExecutionMode}
        
        for task in tasks:
            # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
            execution_mode_count[task.execution_mode] += 1
            
            # ç°¡å˜ãªãƒªã‚½ãƒ¼ã‚¹æ¨å®š
            if task.execution_mode == ExecutionMode.PROCESS:
                total_cpu += 0.5  # ãƒ—ãƒ­ã‚»ã‚¹ã¯å¤šãã®CPUã‚’ä½¿ç”¨
                total_memory += 128  # MB
            elif task.execution_mode == ExecutionMode.THREAD:
                total_cpu += 0.2
                total_memory += 64
            else:  # ASYNC
                total_cpu += 0.1
                total_memory += 32
        
        return {
            'total_cpu_requirement': total_cpu,
            'total_memory_requirement_mb': total_memory,
            'execution_mode_distribution': {
                mode.value: count for mode, count in execution_mode_count.items()
            }
        }
    
    def _calculate_total_execution_time(self, execution_batches: List[List[ExecutionTask]]) -> float:
        """ç·å®Ÿè¡Œæ™‚é–“è¨ˆç®—"""
        total_time = 0.0
        
        for batch in execution_batches:
            if not batch:
                continue
            
            # ãƒãƒƒãƒå†…ã®æœ€å¤§å®Ÿè¡Œæ™‚é–“ï¼ˆä¸¦åˆ—å®Ÿè¡Œã®ãŸã‚ï¼‰
            batch_max_time = max(task.timeout for task in batch)
            total_time += batch_max_time
        
        return total_time
    
    def _calculate_parallel_factor(self, execution_batches: List[List[ExecutionTask]]) -> float:
        """ä¸¦åˆ—åº¦è¨ˆç®—"""
        if not execution_batches:
            return 1.0
        
        total_tasks = sum(len(batch) for batch in execution_batches)
        total_batches = len(execution_batches)
        
        # å¹³å‡ãƒãƒƒãƒã‚µã‚¤ã‚º = ä¸¦åˆ—åº¦ã®æŒ‡æ¨™
        return total_tasks / total_batches if total_batches > 0 else 1.0
    
    def _generate_optimization_notes(
        self, 
        execution_batches: List[List[ExecutionTask]], 
        resource_requirements: Dict[str, Any]
    ) -> List[str]:
        """æœ€é©åŒ–ææ¡ˆç”Ÿæˆ"""
        notes = []
        
        # ãƒãƒƒãƒæ•°ãƒã‚§ãƒƒã‚¯
        if len(execution_batches) > 5:
            notes.append("å®Ÿè¡ŒãƒãƒƒãƒãŒå¤šã„ãŸã‚ã€ä¾å­˜é–¢ä¿‚ã®ç°¡ç´ åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
        if resource_requirements.get('total_cpu_requirement', 0) > 2.0:
            notes.append("CPUè¦æ±‚é‡ãŒé«˜ã„ãŸã‚ã€ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—æ•°ã®èª¿æ•´ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        if resource_requirements.get('total_memory_requirement_mb', 0) > 1024:
            notes.append("ãƒ¡ãƒ¢ãƒªè¦æ±‚é‡ãŒé«˜ã„ãŸã‚ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆ†å¸ƒãƒã‚§ãƒƒã‚¯
        mode_dist = resource_requirements.get('execution_mode_distribution', {})
        if mode_dist.get('process', 0) > 3:
            notes.append("ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ãŒå¤šã„ãŸã‚ã€ã‚¹ãƒ¬ãƒƒãƒ‰ä¸¦åˆ—ã¸ã®å¤‰æ›´ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        return notes
    
    def submit_task(self, task: ExecutionTask) -> str:
        """ã‚¿ã‚¹ã‚¯æŠ•å…¥"""
        with self.scheduler_lock:
            self.task_counter += 1
            if not task.task_id:
                task.task_id = f"task_{self.task_counter}_{task.strategy_name}"
            
            self.pending_tasks.append(task)
            self.performance_metrics['total_tasks_submitted'] += 1
            
            logger.info(f"Task submitted: {task.task_id} ({task.strategy_name}, {task.execution_mode.value})")
            return task.task_id
    
    def execute_tasks(self, tasks: List[ExecutionTask]) -> Dict[str, ExecutionResult]:
        """ã‚¿ã‚¹ã‚¯ç¾¤å®Ÿè¡Œ"""
        logger.info(f"Executing {len(tasks)} tasks")
        
        with self.scheduler_lock:
            self.state = SchedulerState.SCHEDULING
        
        try:
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°è¨ˆç”»ä½œæˆ
            plan = self.create_scheduling_plan(tasks)
            
            with self.scheduler_lock:
                self.state = SchedulerState.EXECUTING
            
            # ãƒãƒƒãƒã”ã¨ã«å®Ÿè¡Œ
            results = {}
            for batch_index, batch in enumerate(plan.execution_batches):
                logger.info(f"Executing batch {batch_index + 1}/{len(plan.execution_batches)} with {len(batch)} tasks")
                
                batch_results = self._execute_batch(batch)
                results.update(batch_results)
                
                # ãƒãƒƒãƒé–“ã®å¾…æ©Ÿæ™‚é–“
                if batch_index < len(plan.execution_batches) - 1:
                    time.sleep(self.config.get('scheduling', {}).get('task_submission_interval', 0.1))
            
            # å®Ÿè¡Œå±¥æ­´è¨˜éŒ²
            self.execution_history.append({
                'timestamp': datetime.now().isoformat(),
                'task_count': len(tasks),
                'execution_time': sum(r.execution_time for r in results.values()),
                'success_count': sum(1 for r in results.values() if r.status == ExecutionStatus.COMPLETED),
                'failure_count': sum(1 for r in results.values() if r.status == ExecutionStatus.FAILED)
            })
            
            with self.scheduler_lock:
                self.state = SchedulerState.IDLE
            
            logger.info(f"Task execution completed: {len(results)} results")
            return results
        
        except Exception as e:
            logger.error(f"Task execution failed: {e}")
            with self.scheduler_lock:
                self.state = SchedulerState.IDLE
            raise
    
    def _execute_batch(self, batch: List[ExecutionTask]) -> Dict[str, ExecutionResult]:
        """ãƒãƒƒãƒå®Ÿè¡Œ"""
        futures: Dict[str, Future] = {}
        results: Dict[str, ExecutionResult] = {}
        
        # ãƒãƒƒãƒå†…ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—æŠ•å…¥
        for task in batch:
            try:
                executor = self.executors[task.execution_mode]
                future = executor.submit_task(task)
                futures[task.task_id] = future
                
                with self.scheduler_lock:
                    self.running_tasks[task.task_id] = task
                
            except Exception as e:
                logger.error(f"Failed to submit task {task.task_id}: {e}")
                results[task.task_id] = ExecutionResult(
                    task_id=task.task_id,
                    strategy_name=task.strategy_name,
                    status=ExecutionStatus.FAILED,
                    error=str(e)
                )
        
        # ãƒãƒƒãƒå®Œäº†å¾…æ©Ÿ
        completed_futures = as_completed(futures.values(), 
                                       timeout=max(task.timeout for task in batch) if batch else 300)
        
        for future in completed_futures:
            # Futureâ†’TaskIDãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ¢ã™
            task_id = None
            for tid, f in futures.items():
                if f is future:
                    task_id = tid
                    break
            
            if not task_id:
                continue
            
            task = self.running_tasks.get(task_id)
            if not task:
                continue
            
            # çµæœå‡¦ç†
            start_time = datetime.now()
            try:
                result_value = future.result()
                end_time = datetime.now()
                execution_time = (end_time - start_time).total_seconds()
                
                result = ExecutionResult(
                    task_id=task_id,
                    strategy_name=task.strategy_name,
                    status=ExecutionStatus.COMPLETED,
                    result=result_value,
                    execution_time=execution_time,
                    start_time=start_time,
                    end_time=end_time
                )
                
                self.performance_metrics['total_tasks_completed'] += 1
                
            except Exception as e:
                end_time = datetime.now()
                execution_time = (end_time - start_time).total_seconds()
                
                result = ExecutionResult(
                    task_id=task_id,
                    strategy_name=task.strategy_name,
                    status=ExecutionStatus.FAILED,
                    error=str(e),
                    execution_time=execution_time,
                    start_time=start_time,
                    end_time=end_time
                )
                
                self.performance_metrics['total_tasks_failed'] += 1
                logger.error(f"Task {task_id} failed: {e}")
            
            results[task_id] = result
            
            with self.scheduler_lock:
                self.running_tasks.pop(task_id, None)
                self.completed_tasks[task_id] = result
        
        # å¹³å‡å®Ÿè¡Œæ™‚é–“æ›´æ–°
        if results:
            avg_time = sum(r.execution_time for r in results.values()) / len(results)
            current_avg = self.performance_metrics['average_execution_time']
            total_completed = self.performance_metrics['total_tasks_completed']
            
            # ç§»å‹•å¹³å‡è¨ˆç®—
            self.performance_metrics['average_execution_time'] = (
                (current_avg * (total_completed - len(results)) + avg_time * len(results)) / total_completed
            )
        
        return results
    
    def cancel_task(self, task_id: str) -> bool:
        """ã‚¿ã‚¹ã‚¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«"""
        # å®Ÿè¡Œä¸­ã‚¿ã‚¹ã‚¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«
        with self.scheduler_lock:
            task = self.running_tasks.get(task_id)
            if task:
                executor = self.executors[task.execution_mode]
                success = executor.cancel_task(task_id)
                if success:
                    self.running_tasks.pop(task_id, None)
                    self.completed_tasks[task_id] = ExecutionResult(
                        task_id=task_id,
                        strategy_name=task.strategy_name,
                        status=ExecutionStatus.CANCELLED
                    )
                    logger.info(f"Task {task_id} cancelled successfully")
                return success
            
            # å¾…æ©Ÿä¸­ã‚¿ã‚¹ã‚¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«
            for i, pending_task in enumerate(self.pending_tasks):
                if pending_task.task_id == task_id:
                    self.pending_tasks.pop(i)
                    self.completed_tasks[task_id] = ExecutionResult(
                        task_id=task_id,
                        strategy_name=pending_task.strategy_name,
                        status=ExecutionStatus.CANCELLED
                    )
                    logger.info(f"Pending task {task_id} cancelled successfully")
                    return True
        
        return False
    
    def get_execution_status(self) -> Dict[str, Any]:
        """å®Ÿè¡ŒçŠ¶æ…‹å–å¾—"""
        with self.scheduler_lock:
            status = {
                'scheduler_state': self.state.value,
                'pending_tasks': len(self.pending_tasks),
                'running_tasks': len(self.running_tasks),
                'completed_tasks': len(self.completed_tasks),
                'performance_metrics': self.performance_metrics.copy()
            }
            
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¿ã‚¹ã‚¯æ•°çµ±è¨ˆ
            active_by_mode = {}
            for mode, executor in self.executors.items():
                active_by_mode[mode.value] = executor.get_active_task_count()
            
            status['active_tasks_by_mode'] = active_by_mode
            
            return status
    
    def get_task_result(self, task_id: str) -> Optional[ExecutionResult]:
        """ã‚¿ã‚¹ã‚¯çµæœå–å¾—"""
        with self.scheduler_lock:
            return self.completed_tasks.get(task_id)
    
    def shutdown(self):
        """ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
        logger.info("Shutting down Concurrent Execution Scheduler...")
        
        with self.scheduler_lock:
            self.state = SchedulerState.STOPPED
        
        # å…¨å®Ÿè¡Œå™¨ã‚’ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
        for mode, executor in self.executors.items():
            try:
                executor.shutdown()
            except Exception as e:
                logger.error(f"Error shutting down {mode.value} executor: {e}")
        
        logger.info("Concurrent Execution Scheduler shutdown complete")

# ãƒ‡ãƒ¢ç”¨é–¢æ•°
def demo_strategy_function(strategy_name: str, duration: float = 1.0) -> Dict[str, Any]:
    """ãƒ‡ãƒ¢ç”¨æˆ¦ç•¥é–¢æ•°"""
    import time
    import random
    
    print(f"Executing {strategy_name} for {duration}s...")
    time.sleep(duration)
    
    # ãƒ©ãƒ³ãƒ€ãƒ ãªçµæœã‚’ç”Ÿæˆ
    result = {
        'strategy_name': strategy_name,
        'execution_duration': duration,
        'trades_executed': random.randint(1, 10),
        'pnl': round(random.uniform(-100, 200), 2),
        'success': random.choice([True, True, True, False])  # 75% success rate
    }
    
    if not result['success']:
        raise Exception(f"Strategy {strategy_name} failed with random error")
    
    return result

if __name__ == "__main__":
    # ãƒ‡ãƒ¢å®Ÿè¡Œ
    print("=" * 60)
    print("Concurrent Execution Scheduler - Demo")
    print("=" * 60)
    
    try:
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼åˆæœŸåŒ–
        scheduler = ConcurrentExecutionScheduler()
        
        # ãƒ‡ãƒ¢ã‚¿ã‚¹ã‚¯ä½œæˆ
        demo_tasks = [
            ExecutionTask(
                task_id=f"demo_task_{i}",
                strategy_name=f"Strategy{i}",
                execution_mode=ExecutionMode.THREAD if i % 3 == 0 
                              else ExecutionMode.PROCESS if i % 3 == 1 
                              else ExecutionMode.ASYNC,
                function=demo_strategy_function,
                args=(f"Strategy{i}", 2.0 + i * 0.5),
                priority=3 - (i % 3),
                timeout=30.0
            )
            for i in range(1, 6)
        ]
        
        print(f"\nğŸ¯ Testing concurrent execution with {len(demo_tasks)} tasks")
        for task in demo_tasks:
            print(f"  - {task.strategy_name}: {task.execution_mode.value} mode, priority {task.priority}")
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°è¨ˆç”»ä½œæˆ
        print(f"\nğŸ“‹ Creating scheduling plan...")
        plan = scheduler.create_scheduling_plan(demo_tasks)
        
        print(f"Execution Batches: {len(plan.execution_batches)}")
        print(f"Total Estimated Time: {plan.total_estimated_time:.1f}s")
        print(f"Parallel Factor: {plan.parallel_factor:.2f}")
        
        if plan.optimization_notes:
            print(f"Optimization Notes:")
            for note in plan.optimization_notes:
                print(f"  ğŸ’¡ {note}")
        
        # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
        print(f"\nğŸš€ Executing tasks...")
        start_time = time.time()
        results = scheduler.execute_tasks(demo_tasks)
        execution_time = time.time() - start_time
        
        print(f"\nğŸ“Š Execution Results (completed in {execution_time:.2f}s):")
        print("-" * 50)
        
        for task_id, result in results.items():
            status_emoji = "âœ…" if result.status == ExecutionStatus.COMPLETED else "âŒ"
            print(f"{status_emoji} {result.strategy_name} ({task_id})")
            print(f"    Status: {result.status.value}")
            print(f"    Execution Time: {result.execution_time:.2f}s")
            if result.error:
                print(f"    Error: {result.error}")
            elif result.result:
                print(f"    Result: {result.result}")
        
        # å®Ÿè¡ŒçŠ¶æ…‹çµ±è¨ˆ
        status = scheduler.get_execution_status()
        print(f"\nğŸ“ˆ Scheduler Statistics:")
        print(f"  Completed Tasks: {status['completed_tasks']}")
        print(f"  Performance Metrics:")
        metrics = status['performance_metrics']
        print(f"    Total Submitted: {metrics['total_tasks_submitted']}")
        print(f"    Total Completed: {metrics['total_tasks_completed']}")
        print(f"    Total Failed: {metrics['total_tasks_failed']}")
        print(f"    Average Execution Time: {metrics['average_execution_time']:.2f}s")
        
        print("\nâœ… Concurrent Execution Scheduler demo completed successfully!")
        
    except Exception as e:
        print(f"\nâŒ Demo failed: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if 'scheduler' in locals():
            scheduler.shutdown()
